# LLM
Educational PyTorch implementation demonstrating the first 3 layers of a GPT-style Transformer model. Includes tokenization, embeddings, sinusoidal positional encoding, multi-head self-attention. Designed for learning and visualizing how input text is processed through a Transformer architecture.
